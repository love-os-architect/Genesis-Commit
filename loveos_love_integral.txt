{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# LOVE\u2011OS Love Integral Notebook\n\nThis notebook computes **alignment (R)**, **meaning density (M\u0304)**, **friction (C\u0304)**, **subjective time density (\u03c4\u0307)**, **criticality (s)** and the **Love Integral (\u222bL dt)** from a daily CSV log.\n\n> **Note**: This framework does **not** change physical time. It quantifies **subjective time density** and the **amount of order (negentropy) you drive** through actions.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": "\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nfrom pathlib import Path\nplt.rcParams.update({'font.family':'DejaVu Sans','axes.unicode_minus':False})\n\nCSV_PATH = Path('loveos_love_integral_example.csv')  # change to your file or to 'loveos_love_integral_template.csv' after filling\nassert CSV_PATH.exists(), f\"CSV not found: {CSV_PATH}\"\nraw = pd.read_csv(CSV_PATH)\nraw['date'] = pd.to_datetime(raw['date'])\nraw = raw.sort_values('date').reset_index(drop=True)\nraw.head()\n"}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": "\nimport numpy as np\n\ndef compute_attention_times(row):\n    cols = ['t1_time_min','t2_time_min','t3_time_min','t4_time_min','t5_time_min']\n    times = row[cols].fillna(0).values.astype(float)\n    total = times.sum()\n    if total <= 0:\n        a = np.ones_like(times)/len(times)\n    else:\n        a = times/total\n    return a\n\n# Alignment (R) using L2-spikiness normalization\nfrom math import sqrt\n\ndef alignment_R(a):\n    N = len(a)\n    l2 = np.linalg.norm(a,2)\n    return float((l2 - 1/sqrt(N))/(1 - 1/sqrt(N))) if N>1 else 1.0\n\n# Meaning density (Mbar) using task meanings m_ti (1-7)\nM_cols = ['m_t1','m_t2','m_t3','m_t4','m_t5']\n\ndef meaning_bar(row, a):\n    m = row[M_cols].fillna(0).values.astype(float)\n    return float((a*m).sum())\n\n# Friction Cbar from interruptions / tabs / stress\n# Normalize by robust scale (95th percentile) to 0-1, then map to 0-7\n\ndef friction_bar(df):\n    eps=1e-9\n    ints = df['interruptions'].astype(float)\n    tabs = df['tabs_open'].astype(float)\n    stress = df['stress_1to7'].astype(float)/7.0\n    s_int = ints / max(ints.quantile(0.95), eps)\n    s_tabs = tabs / max(tabs.quantile(0.95), eps)\n    C01 = 0.5*stress + 0.3*s_int + 0.2*s_tabs\n    return 7.0*np.clip(C01,0,1)\n\n# Subjective time density tau_dot\nalpha0, aR, aC, aM = 1.0, 1.2, 0.2, 0.25\n\ndef tau_dot_series(R, Cbar, Mbar):\n    return (alpha0 + aR*R) * (1 - aC*Cbar/7.0) * (1 + aM*Mbar/7.0)\n\n# Progress speed v_parallel and LoveForce L\n\ndef loveforce(progress, R, Cbar):\n    v = np.gradient(progress)              # day-to-day speed along meaning axis (proxy)\n    mu = 0.8 + 0.5*(Cbar/7.0) + 0.4*(1-R)  # inertia\n    a_par = np.gradient(v)\n    L = mu * a_par\n    return v, a_par, mu, L\n\n# Criticality s (S-curve)\nfrom math import exp\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\nkappa, tau_c = 6.0, 0.9\nwR, wM, wC = 0.7, 0.4, 0.6\n\ndef criticality_s(R, Mbar, Cbar):\n    z = kappa*(wR*R + wM*(Mbar/7.0) - wC*(Cbar/7.0) - tau_c)\n    return sigmoid(z)\n"}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": "\n# Compute per-day metrics\nA_list, R_list, Mbar_list = [], [], []\nfor _, row in raw.iterrows():\n    a = compute_attention_times(row)\n    A_list.append(a)\n    R_list.append(alignment_R(a))\n    Mbar_list.append(meaning_bar(row, a))\n\nR = np.array(R_list)\nMbar = np.array(Mbar_list)\nCbar = friction_bar(raw).values\n\ntau_dot = tau_dot_series(R, Cbar, Mbar)\nprogress = raw['progress_score_0to1'].astype(float).values\nv, a_par, mu, L = loveforce(progress, R, Cbar)\n\ns = criticality_s(R, Mbar, Cbar)\n\nmetrics = raw[['date','phase']].copy() if 'phase' in raw.columns else raw[['date']].copy()\nmetrics['R_alignment'] = R\nmetrics['Mbar_meaning'] = Mbar\nmetrics['Cbar_friction'] = Cbar\nmetrics['tau_dot'] = tau_dot\nmetrics['v_parallel'] = v\nmetrics['a_parallel'] = a_par\nmetrics['mu_inertia'] = mu\nmetrics['LoveForce_L'] = L\nmetrics['criticality_s'] = s\nmetrics.to_csv('loveos_love_integral_metrics.csv', index=False)\nmetrics.head()\n"}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": "\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\n\nfig, ax = plt.subplots(figsize=(11,5))\nax.plot(metrics['date'], metrics['tau_dot'], lw=2, label='Subjective time density (\u03c4\u0307)')\nax.plot(metrics['date'], metrics['v_parallel'], lw=2, label='Meaning-aligned speed (v\u2225)')\nax.set_title('Subjective time density & action speed (daily)')\nax.set_xlabel('Date'); ax.set_ylabel('Index (normalized)')\nax.legend(); ax.grid(alpha=0.3)\nfig.autofmt_xdate(); fig.tight_layout(); fig.savefig('plot_tau_v.png', dpi=160)\n\nfig, ax = plt.subplots(figsize=(11,5))\nax.plot(metrics['date'], metrics['LoveForce_L'], lw=2, label='LoveForce L')\nax.plot(metrics['date'], metrics['criticality_s'], lw=2, label='Criticality s (0-1)')\nax.set_title('LoveForce and criticality over time')\nax.set_xlabel('Date'); ax.set_ylabel('Index')\nax.legend(); ax.grid(alpha=0.3)\nfig.autofmt_xdate(); fig.tight_layout(); fig.savefig('plot_L_s.png', dpi=160)\n\n# Heatmap of attention distribution (percentage per task)\nA = np.vstack(A_list)\nif A.shape[1] == 5:\n    fig, ax = plt.subplots(figsize=(11,3.6))\n    im = ax.imshow(A.T, aspect='auto', cmap='Blues', vmin=0, vmax=1)\n    ax.set_yticks(range(5)); ax.set_yticklabels(['t1','t2','t3','t4','t5'])\n    ax.set_xticks(range(len(metrics['date'])));\n    ax.set_xticklabels([d.strftime('%m-%d') for d in metrics['date']])\n    ax.set_title('Attention distribution per day')\n    plt.colorbar(im, ax=ax, label='Share of time')\n    fig.tight_layout(); fig.savefig('heatmap_attention.png', dpi=160)\n\n# Weekly Love Integral (sum of L)\nweek = metrics.copy()\nweek['week'] = week['date'].dt.to_period('W').astype(str)\nagg = week.groupby('week')['LoveForce_L'].sum().reset_index()\nfig, ax = plt.subplots(figsize=(8,4))\nax.bar(agg['week'], agg['LoveForce_L'], color='#1f77b4')\nax.set_title('Weekly Love Integral (\u03a3 L)')\nax.set_xlabel('Week'); ax.set_ylabel('Sum of L')\nax.grid(axis='y', alpha=0.3); fig.tight_layout(); fig.savefig('bar_weekly_I.png', dpi=160)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## How to use this notebook\n1. Fill **`loveos_love_integral_template.csv`** (one row per day). Minimum required:\n   - `date`, task times `t1_time_min`..`t5_time_min`, meanings `m_t1`..`m_t5`,\n     `interruptions`, `tabs_open`, `stress_1to7`, and `progress_score_0to1`.\n2. Set `CSV_PATH` at the top of the notebook to your filled CSV file.\n3. Re\u2011run all cells. Outputs:\n   - `loveos_love_integral_metrics.csv` (daily metrics)\n   - Figures: `plot_tau_v.png`, `plot_L_s.png`, `heatmap_attention.png`, `bar_weekly_I.png`.\n\n**Notes**\n- All labels are in English to avoid font issues.\n- This model measures **subjective time density** and **order driven by actions**, not physical time.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}
